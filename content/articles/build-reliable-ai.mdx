---
seo:
  title: 'Zero-Hallucination AI: Reliable Assistants for High-Stakes Domains'
  description: >-
    Guide to crush AI hallucinations with RAG, guardrails, benchmarks,
    DevSecOps. Ship safe assistants for finance, health, and compliance.
title: Building a Hallucination-Proof AI Assistant
subTitle: "AI **hallucinations** happen when a model confidently supplies information that isn’t real, misquoting regulations, inventing statistics, or citing studies that never existed. In low-stakes chat apps it’s an annoyance; in finance, healthcare, legal, or compliance it’s existential risk.\n\nIn this article, we will show you how to ship an assistant that handles hallucinations with layered safeguards and measurable benchmarks.\n\n## Why High-Stakes Domains Beat Up Chatbots\n\nOne wrong answer can trigger:\n\n* regulatory fines or legal action\n* patient harm or misdiagnosis\n* financial losses and reputational damage\n\nAssistants in these settings must parse dense rules, handle edge cases, and **never guess**. The error budget is effectively **zero**.\n\n## Three-Layer Defense Against Hallucination\n\n| Layer                                    | What It Does                                                                                                                                                                        | Win / Loss                                                                      | Examples                                                                                                                                                                                                                                                                                                      |\n| ---------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Retrieval-Augmented Generation (RAG)** | Strengthens answers with authoritative ground truth. Pulls fresh text from trusted sources like regulatory sites, peer-reviewed papers, internal SOPs *before* composing an answer. | ✅ Grounds replies in evidence. ❌ Fails if retrieval fetches the wrong document. | A company [built a support bot](https://arxiv.org/abs/2404.17723) that queries past tickets and knowledge bases via RAG/KG. It cut resolution time by \\~29% and improved accuracy significantly (MRR +77%)                                                                                                    |\n| **Guardrail Filter**                     | Post-processes every answer: blocks missing citations, scope creep (e.g., medical or legal advice), and hand-wavy *always/never* claims.                                            | ✅ Cuts risky output. ❌ Over-filters if rules are sloppy.                        | An [online banking assistant](https://aws.amazon.com/blogs/machine-learning/build-safe-and-responsible-generative-ai-applications-with-guardrails) uses output guardrails to block advice on illegal investments, speculative statements like “always invest in X” and hate speech or inappropriate language. |\n| **Question Sanitizer**                   | Rewrites the user prompt to remove ambiguity and hidden assumptions.                                                                                                                | ✅ Sharper queries; cleaner answers. ❌ Needs solid NLU to keep the chat natural. | *Raw user input:* ~~“Is this drug safe for kids?~~ Sanitized prompt: “According to current Therapeutic Goods Administration guidelines, what is the approved dosage and contraindication list for \\[Drug X] in children aged 6–12?”                                                                           |\n\n> **Rule of thumb:** Use all three, one patch isn’t enough.\n\n## Reference Architecture\n\n![](/images/ssw-web/blog/reference_architecture.png \"This diagram shows the flow of a user’s question through the system. The assistant first cleans up the question, then looks up reliable sources before generating a response. That answer is checked for risky content, logged for review, and only then sent back to the user. It’s built to reduce mistakes and make sure every response is backed by real evidence.\")\n\n### Components & Best Practices\n\n* **Vector Store & Embeddings**\n  Use top-tier embedding models benchmarked on [MTEB](https://huggingface.co/spaces/mteb/leaderboard) (e.g., Snowflake Arctic, Jasper/Stella) for recall and speed. Keep vector DB options flexible: FAISS for self-hosting, Pinecone for scale, Azure Cognitive Search for ACLs.\n* **Retrieval Tuning**\n  Evaluate with metrics like recall\\@k, MRR, NDCG, and validate retriever+chunking combos per established frameworks.\n* **Foundation Model & Versioning**\n  For the latest AI models, head to [LiveBench](https://livebench.ai/#/). Record the *model hash* in every run.\n* **Guardrails**\n  Use both rule-based and model-based approaches: OpenAI guardrails, Microsoft AutoGen, Nvidia Guardrails, or regex for citations/scope checks.\n* **Audit Logging**\n  Append-only logs (e.g., Cosmos, DynamoDB, Postgres WAL) should capture prompts, retrieval IDs, model info, and guardrail decisions.\n\n### \U0001F527 Implementation with AutoGen + Python\n\n```python\nfrom autogen import AssistantConfig, RetrievalTool, AgentExecutor\n\n# 1. RetrievalTool setup (RAG)\nretriever = RetrievalTool(\n    index_name=\"domain_knowledge\",\n    sources=[\n        :contentReference[oaicite:2]{index=2}\n        :contentReference[oaicite:3]{index=3}\n        :contentReference[oaicite:4]{index=4}\n    ],\n    :contentReference[oaicite:5]{index=5}\n)\n```\n\n* **RetrievalTool** acts as your RAG layer. It fetches top-*k* relevant docs before calling the LLM. Use `top_k≈5` as testing shows both precision and speed are reasonable.\n\n```python\nconfig = AssistantConfig(\n    model=\"gpt‑4.1‑mini\",\n    tools=[retriever],\n    system_prompt=(\n        \"You are a compliance assistant. \"\n        \"Cite every fact with source_URL. \"\n        \"If unsure, answer 'I don't know'.\"\n    ),\n)\n```\n\n* **AssistantConfig** wires the LLM and tools together.\n* `system_prompt` sets strict rules: always cite, default to “I don’t know.” This is your first line of defense against hallucination.\n\n```python\nbot = AgentExecutor(config)\n\nprint(bot.run(\"Capital gains tax exemption for small businesses?\"))\n```\n\n* **AgentExecutor** launches the assistant as configured: sanitized question → fetch docs → generate answer → guardrail check (see below) → response with citations.\n\n***\n\n### \U0001F6E1️ Guardrails (Simplest Version)\n\n```python\nimport re\n\ndef guardrail(reply: str) -> str | None:\n    # 1️⃣ Block replies with no URLs (no citations)\n    if \"http\" not in reply:\n        return None\n\n    # 2️⃣ Flag overconfident claims\n    if re.search(r\"\\b(always|guarantee|never)\\b\", reply, re.I):\n        reply += \"\\n\\n*Confirm with a certified professional before relying on this information.*\"\n\n    return reply\n```\n\n* Rejects uncited claims (avoids hallucinations).\n* Softens absolutist language (reduces legal/medical risk).\n* Returns `None` if filtered out, letting your API gateway handle fallback.\n\n***\n\n### \U0001F6E0️ Using AutoGen’s Retrieval-Augmented Agents\n\nAutoGen’s built-in pattern with `AssistantAgent` + `RetrieveUserProxyAgent` does similar work:\n\n```python\n# Setup assistant agent\nassistant = AssistantAgent(\n    name=\"assistant\",\n    :contentReference[oaicite:13]{index=13}\n)\n\n# Setup retrieval proxy\nrag = RetrieveUserProxyAgent(\n    name=\"rag\",\n    human_input_mode=\"NEVER\",\n    retrieve_config={\n        \"docs_path\": [\"...\"],\n        \"chunk_token_size\": 2000,\n        :contentReference[oaicite:14]{index=14}\n        :contentReference[oaicite:15]{index=15}\n    }\n)\n\n# Kick off RAG-augmented chat\n:contentReference[oaicite:16]{index=16}\n```\n\n**What this does:**\n\n1. **RetrieveUserProxyAgent** fetches documents relevant to your query.\n2. It passes retrieved content to **AssistantAgent**.\n3. You get an answer grounded in real text—no hallucination (assuming retrieval is correct).\n\n### Summary\n\n* **RetrievalTool** + **AssistantConfig** = your RAG layer\n* **AgentExecutor** runs the loop and plugs into guardrail\n* Or, use **AssistantAgent** + **RetrieveUserProxyAgent** combo for multi-agent RAG\n* Add guardrails to enforce citations and soften risky phrasing\n* Log all decisions for auditability\n\n## Measurement is Mandatory \U0001F9EA\n\nTrack these from Day 0:\n\n* **Exact‑Answer Accuracy** (strict human match)\n* **Citation Coverage** (every claim cited)\n* **Compliance Errors** (rules/dosage mismatches)\n* **Hallucination Rate** (uncited claims)\n* **Retrieval Miss Rate** (monitor index drift or ACL failures)\n\nUse CI tests:\n\n```python\nfor q, gold in dataset:\n    response = bot.run(q)\n    score = compare_answers(response, gold, rubric=\"strict\")\n    assert score >= 0.95\n```\n\n## Scaling Strategy\n\n| Stage           | Accuracy Target        | Traffic | Human-in-Loop        |\n| --------------- | ---------------------- | ------- | -------------------- |\n| Shadow Mode     | ≥ 80 % observed        | 0 %     | 100 % offline review |\n| Pilot/Augment   | ≥ 80 %                 | \\~5 %   | Mandatory review     |\n| Limited Release | ≥ 95 % on top queries  | \\~25 %  | Spot check           |\n| Full Automation | ≥ 99 % + zero critical | 100 %   | Exception only       |\n\nAuto-fallback to human if metrics dip.\n\n## Domain Experts Are Non-Negotiable\n\n* **Source Curation** – SMEs tag gold paragraphs; retriever ignores the rest.\n* **Prompt Reviews** – experts catch edge cases outsiders miss.\n* **Error Triage** – every bad answer is labeled *why* it failed, not just “wrong.”\n\nTreat specialists as co-developers, not QA afterthoughts.\n\n## Key Takeaways\n\n1. **Layer it on** – RAG + sanitization + guardrail + hallucination detection.\n2. **Measure everything** – strict metrics are non-negotiable.\n3. **Domain experts drive development**, not just QA.\n4. **Secure + log by default**, with ACLs and audit trails.\n5. **Scale with care** – stay in human-in-the-loop until reliability is proven.\n\nNail these, and you’ll move from a flashy demo to a production-grade AI advisor that never makes up the rules.\n"
author: content/presenters/calum-simpson.mdx
sidebarPanel:
  title: 2-Day Pre-Migration Assessment Engagement
  description: >-
    Get a solid foundation for your .NET 8 migration project, ensuring you are
    well-prepared to tackle the migration with confidence.
  showSidebarPanel: true
  actionUrl: /
  actionText: Learn more
callToAction:
  title: Talk to us about your project
  subTitle: Connect with our Account Managers to discuss how we can help.
  showCallToAction: true
  buttonText: Book a FREE Initial Meeting
  animated: true
  buttonSubtitle: or call +61 2 9953 3000
---

